{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27a51dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec278f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/',download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e45b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459cf717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=MNIST(root='data/',train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcd6cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x23D89BC2F40>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480052d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a97ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label=dataset[0]\n",
    "plt.imshow(image,cmap='gray')\n",
    "print('Label:',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2b3a2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab01ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfce998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1cc21f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bde1dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_ds,val_ds=random_split(dataset,[50000,10000])\n",
    "len(train_ds),len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b605a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=100\n",
    "train_loader=DataLoader(train_ds,batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_ds,batch_size)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dead09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size=28*28\n",
    "num_classes=10\n",
    "\n",
    "#logistics Regresseion model\n",
    "model=nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7d911ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0261,  0.0345,  0.0345,  ..., -0.0197, -0.0220,  0.0134],\n",
       "        [-0.0062,  0.0152,  0.0197,  ...,  0.0259,  0.0040,  0.0298],\n",
       "        [-0.0141,  0.0236,  0.0140,  ...,  0.0293, -0.0093, -0.0062],\n",
       "        ...,\n",
       "        [-0.0085, -0.0058,  0.0159,  ...,  0.0137, -0.0168,  0.0120],\n",
       "        [ 0.0199,  0.0309,  0.0261,  ...,  0.0173, -0.0146, -0.0095],\n",
       "        [-0.0210,  0.0236, -0.0210,  ...,  0.0030,  0.0272,  0.0032]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59c7182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0178,  0.0095,  0.0150, -0.0207,  0.0220, -0.0089,  0.0308, -0.0286,\n",
       "        -0.0046, -0.0223], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d4231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8, 3, 7, 1, 2, 4, 2, 1, 7, 4, 5, 2, 0, 1, 1, 4, 3, 7, 3, 4, 9, 2, 8,\n",
      "        7, 0, 3, 4, 1, 7, 3, 2, 3, 3, 7, 6, 5, 0, 1, 3, 7, 9, 6, 8, 9, 1, 4, 6,\n",
      "        9, 2, 1, 8, 4, 9, 4, 3, 9, 9, 8, 7, 4, 6, 1, 9, 3, 0, 1, 7, 3, 1, 6, 9,\n",
      "        2, 8, 7, 9, 0, 1, 7, 2, 9, 8, 3, 0, 4, 9, 4, 7, 5, 5, 4, 0, 0, 3, 3, 0,\n",
      "        3, 8, 6, 8])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10800/1974446091.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abfd31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding layer to the model\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9cc1356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd1cfc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0226,  0.0037, -0.0221,  ...,  0.0090,  0.0234, -0.0279],\n",
       "         [-0.0321,  0.0234,  0.0325,  ..., -0.0324,  0.0037,  0.0194],\n",
       "         [-0.0248,  0.0021, -0.0087,  ..., -0.0123,  0.0278,  0.0117],\n",
       "         ...,\n",
       "         [-0.0056,  0.0065, -0.0011,  ..., -0.0080,  0.0171, -0.0053],\n",
       "         [ 0.0049, -0.0036, -0.0221,  ...,  0.0327,  0.0020, -0.0035],\n",
       "         [ 0.0140, -0.0134, -0.0145,  ..., -0.0122,  0.0069, -0.0055]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0144, -0.0329, -0.0199,  0.0077,  0.0080, -0.0254,  0.0259, -0.0082,\n",
       "          0.0349,  0.0298], requires_grad=True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d063de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "outputs.shape :  torch.Size([100, 10])\n",
      "Sample outputs :\n",
      " tensor([[-0.0677, -0.1408, -0.0617,  0.2784, -0.0718, -0.1816,  0.1380,  0.0411,\n",
      "          0.0794,  0.1014],\n",
      "        [ 0.2479,  0.0440, -0.1250,  0.2901,  0.1508,  0.0332,  0.1798,  0.0418,\n",
      "          0.0151,  0.0987]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "208e6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ebe0fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.0915, 0.0851, 0.0921, 0.1294, 0.0912, 0.0817, 0.1125, 0.1021, 0.1061,\n",
      "         0.1084],\n",
      "        [0.1154, 0.0941, 0.0795, 0.1204, 0.1048, 0.0931, 0.1078, 0.0939, 0.0915,\n",
      "         0.0994]])\n",
      "Sum:  0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e4303bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 6, 9, 3, 0, 7, 0, 9, 0, 3, 3, 0, 3, 3, 9, 2, 3, 0, 3, 3, 7, 3, 3,\n",
      "        4, 6, 3, 0, 7, 3, 9, 3, 0, 3, 3, 3, 0, 0, 3, 0, 3, 4, 3, 3, 3, 3, 6, 8,\n",
      "        0, 0, 0, 3, 6, 0, 7, 5, 5, 7, 0, 3, 0, 3, 3, 9, 6, 6, 3, 9, 3, 3, 6, 0,\n",
      "        0, 6, 0, 0, 5, 9, 6, 3, 5, 0, 3, 3, 0, 6, 0, 0, 3, 3, 3, 0, 8, 5, 6, 3,\n",
      "        3, 0, 9, 3])\n",
      "tensor([0.1294, 0.1204, 0.1114, 0.1355, 0.1247, 0.1423, 0.1463, 0.1193, 0.1379,\n",
      "        0.1161, 0.1382, 0.1305, 0.1228, 0.1408, 0.1491, 0.1269, 0.1129, 0.1241,\n",
      "        0.1469, 0.1455, 0.1462, 0.1118, 0.1317, 0.1288, 0.1349, 0.1431, 0.1249,\n",
      "        0.1292, 0.1487, 0.1291, 0.1437, 0.1307, 0.1305, 0.1371, 0.1358, 0.1270,\n",
      "        0.1306, 0.1299, 0.1261, 0.1318, 0.1443, 0.1146, 0.1386, 0.1265, 0.1454,\n",
      "        0.1358, 0.1253, 0.1253, 0.1133, 0.1211, 0.1176, 0.1230, 0.1172, 0.1285,\n",
      "        0.1308, 0.1246, 0.1367, 0.1139, 0.1335, 0.1391, 0.1084, 0.1332, 0.1602,\n",
      "        0.1612, 0.1133, 0.1218, 0.1213, 0.1392, 0.1395, 0.1254, 0.1226, 0.1226,\n",
      "        0.1178, 0.1424, 0.1173, 0.1520, 0.1168, 0.1425, 0.1264, 0.1381, 0.1172,\n",
      "        0.1367, 0.1194, 0.1236, 0.1298, 0.1097, 0.1194, 0.1278, 0.1475, 0.1218,\n",
      "        0.1227, 0.1491, 0.1328, 0.1310, 0.1184, 0.1412, 0.1321, 0.1310, 0.1360,\n",
      "        0.1184], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "705b77d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 8, 6, 3, 0, 7, 2, 4, 5, 5, 3, 9, 1, 9, 0, 5, 1, 9, 6, 7, 0, 1, 5, 5,\n",
       "        7, 5, 5, 9, 0, 6, 0, 8, 1, 7, 7, 0, 4, 3, 3, 1, 3, 8, 2, 3, 3, 4, 8, 0,\n",
       "        1, 8, 8, 7, 9, 1, 2, 8, 2, 9, 0, 7, 1, 5, 0, 3, 5, 6, 9, 4, 2, 5, 6, 1,\n",
       "        1, 6, 1, 1, 6, 8, 8, 3, 8, 7, 4, 3, 5, 1, 1, 4, 5, 9, 4, 7, 0, 1, 6, 0,\n",
       "        7, 1, 3, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0202ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81c55dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1400)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7db3ac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(labels==preds).item() / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f59cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96e5b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2810, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96c6d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "otimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c0afc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func,xb,yb,opt=None,metric=None):\n",
    "    #calculate loss\n",
    "    preds=model(xb)\n",
    "    loss=loss_func(preds,yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        #compute gradients\n",
    "        loss.backward()\n",
    "        #update parameters\n",
    "        opt.step()\n",
    "        #Reset Gradients\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    metric_result=None\n",
    "    if metric is not None:\n",
    "        metric_result=metric(preds,yb)\n",
    "    \n",
    "    return loss.item(),len(xb),metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea15a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn,valid_dl,metric=None):\n",
    "    with torch.no_grad():\n",
    "        #pass each batch through the model\n",
    "        results=[loss_batch(model,loss_fn,xb,yb,metric=metric)\n",
    "                for xb,yb in valid_dl]\n",
    "        #separate losses,counts and metrices\n",
    "        losses,nums,metrics=zip(*results)\n",
    "        #total size of the datset\n",
    "        total=np.sum(nums)\n",
    "        # avg loss across batches\n",
    "        avg_loss=np.sum(np.multiply(losses,nums)) / total\n",
    "        avg_metric=None\n",
    "        if metric is not None:\n",
    "            #avg of metric across all batches\n",
    "            avg_metric=np.sum(np.multiply(metrics,nums)) / total\n",
    "        return avg_loss,total,avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "481ceffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5a33492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3093, Accuracy: 0.0827\n"
     ]
    }
   ],
   "source": [
    "val_loss,total,val_acc=evaluate(model,loss_fn,val_loader,metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91ced2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,loss_fn,opt,train_dl,valid_dl,metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        #training\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model,loss_fn,xb,yb,opt)\n",
    "        #evaluation\n",
    "        result=evaluate(model,loss_fn,valid_dl,metric)\n",
    "        val_loss,total,val_metric = result\n",
    "        \n",
    "        #print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], loss: {:.4f}'\n",
    "                  .format(epoch+1,epochs,val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], loss: {:.4f}, {}: {:.4f}'\n",
    "                 .format(epoch+1,epochs,val_loss,metric.__name__,val_metric))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec24ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine model and optimizer\n",
    "model=MnistModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ad16b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss: 1.8687, accuracy: 0.6754\n",
      "Epoch [2/5], loss: 1.5673, accuracy: 0.7471\n",
      "Epoch [3/5], loss: 1.3582, accuracy: 0.7761\n",
      "Epoch [4/5], loss: 1.2094, accuracy: 0.7941\n",
      "Epoch [5/5], loss: 1.0997, accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43afe8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss: 1.0159, accuracy: 0.8133\n",
      "Epoch [2/5], loss: 0.9502, accuracy: 0.8189\n",
      "Epoch [3/5], loss: 0.8971, accuracy: 0.8233\n",
      "Epoch [4/5], loss: 0.8534, accuracy: 0.8285\n",
      "Epoch [5/5], loss: 0.8167, accuracy: 0.8314\n"
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffe38cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss: 0.7855, accuracy: 0.8347\n",
      "Epoch [2/5], loss: 0.7584, accuracy: 0.8378\n",
      "Epoch [3/5], loss: 0.7349, accuracy: 0.8404\n",
      "Epoch [4/5], loss: 0.7141, accuracy: 0.8423\n",
      "Epoch [5/5], loss: 0.6956, accuracy: 0.8440\n"
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60552852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss: 0.6790, accuracy: 0.8456\n",
      "Epoch [2/5], loss: 0.6640, accuracy: 0.8478\n",
      "Epoch [3/5], loss: 0.6504, accuracy: 0.8492\n",
      "Epoch [4/5], loss: 0.6380, accuracy: 0.8507\n",
      "Epoch [5/5], loss: 0.6267, accuracy: 0.8513\n"
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "774384dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test dataset\n",
    "test_dataset=MNIST(root='data/',\n",
    "                  train=False,\n",
    "                  transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ebeeec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img,model):\n",
    "    xb=img.unsqueeze(0)\n",
    "    yb=model(xb)\n",
    "    _,preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "43b9a023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(img,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8d8059c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5 , Predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets predict\n",
    "img, label = test_dataset[8909]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c41d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
